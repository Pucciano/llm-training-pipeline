# Toolkit zur Erstellung eines LLM-Datensatzes

[![de](https://img.shields.io/badge/lang-de-blue.svg)](./README-DE.md) Â· [![en](https://img.shields.io/badge/lang-en-red.svg)](./README.md)

## ğŸ§  Projektvision

Ziel dieses Projekts ist es, ein qualitativ hochwertigen Datensatz zur Feinabstimmung eines **deutschsprachigen Sprachmodells** zu entwickeln. Viele Open-Source-Modelle sind zwar mehrsprachig trainiert, nutzen intern aber meist Englisch. Der deutschsprachige Raum (DACH) bietet enormes Potenzial â€” sowohl sprachlich als auch thematisch.

Dieses Toolkit begleitet den gesamten Lebenszyklus der Datensatzerstellung: von der manuellen Eingabe Ã¼ber den Massenimport bis zur automatisierten Extraktion von Metadaten.

---

## ğŸ” Funktionsumfang

- âœ… Kommandozeilentool zur strukturierten Eingabe von `instruction`, `input`, `output`, `tags`, `source`, `license`
- âœ… Excel-basierter Bulk-Import und Konvertierung in HuggingFace-kompatibles `.jsonl`-Format
- âœ… Umwandlung von PDF-Dokumenten in Markdown (`pymupdf4llm`)
- âœ… Extraktion von PDF-Metadaten inkl. MD5-Hash, Autor, Lizenz, Seitenanzahl usw.
- âœ… JSONL-DatensÃ¤tze fÃ¼r Befehlsabgestimmte Sprachmodelle (`<thinking>`)
- âœ… Mehrsprachige UnterstÃ¼tzung mit vollstÃ¤ndiger RÃ¼ckverfolgbarkeit der Quellen
- ğŸ› ï¸ Web-UI zur komfortablen Erweiterung und Validierung

---

## ğŸ“¦ Datensatzformat

Der Datensatz wird im `.jsonl`-Format gespeichert (eine Zeile pro Eintrag). Es eignet sich besonders fÃ¼r instruction-tuned generative Modelle wie **Gemma 3B**, **Qwen**, **Mistral** u.â€¯a.

### Beispiel-Datensatz:

```json
{
  "instruction": "ErklÃ¤re die Unterschiede zwischen TCP und UDP.",
  "input": "",
  "output": "TCP ist verbindungsorientiert ...",
  "tags": ["Netzwerktechnik", "Informatik"],
  "source": "OpenBook: Netzwerke leicht erklÃ¤rt",
  "license": "CC-BY-SA 4.0",
  "created_at": "2025-06-03T22:26:29.342385+00:00"
}
````

---

## ğŸš€ Roadmap

1. ğŸ“ Strukturierte Dateneingabe & Bulk-Import
2. ğŸª„ Markdown-Erzeugung aus PDF-Quellen
3. ğŸ“¤ Metadatenexport mit Hash-ÃœberprÃ¼fung
4. ğŸ”„ Validierung und Duplikaterkennung
5. ğŸ“¡ Upload des Datensets als HuggingFace-Dataset
6. ğŸ§ª Validierung und Erweiterung mit Testbeispielen
7. ğŸ”¬ LLM-Training mit LoRA/QLoRA/DPO
8. ğŸŒ Webinterface fÃ¼r die manuelle Datenpflege

---

## ğŸ“‚ Projektstruktur

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf/                # Quell-PDFs
â”‚   â”œâ”€â”€ markdown/           # Generierte Markdown-Dateien
â”‚   â”œâ”€â”€ dataset.jsonl       # Trainingsdaten im JSONL-Format
â”‚   â””â”€â”€ pdf_metadata.jsonl  # Extrahierte PDF-Metadaten
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_dataset_entry.py
â”‚   â”œâ”€â”€ excel_to_dataset.py
â”‚   â”œâ”€â”€ pdf_to_markdown.py
â”‚   â””â”€â”€ generate_pdf_metadata.py
â”œâ”€â”€ README.md
â””â”€â”€ README-DE.md
```

---

## âœ… TODO

* [ ] Web-Interface zur Dateneingabe und Validierung
* [ ] YAML-zu-JSONL-Konvertierung fÃ¼r andere LLM-Formate
* [ ] HuggingFace `datasets`-kompatibler Loader
* [ ] DuplikatprÃ¼fung anhand von MD5/Filepath
* [ ] Quellen- und Lizenz-Helfer fÃ¼r Bulk-Importe
* [ ] Validierung und Testschema fÃ¼r DatensÃ¤tze
* [ ] Automatische Aufteilung in train/valid/test