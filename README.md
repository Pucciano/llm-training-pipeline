# LLM Training Pipeline

[![de](https://img.shields.io/badge/lang-de-blue.svg)](./README-DE.md) Â· [![en](https://img.shields.io/badge/lang-en-red.svg)](./README.md)

## ğŸ§  Project Vision

The goal of this project is to build a high-quality dataset for training a German-language large language model (LLM). While most open-source LLMs are trained multilingually, they often use English internally as a primary language. However, the DACH region (Germany, Austria, Switzerland) holds massive untapped linguistic and domain-specific potential.

This toolkit supports the full dataset lifecycle â€” from structured manual data entry to scalable bulk imports and automated metadata extraction.

---

## ğŸ” Features

- âœ… Command-line interface for structured dataset entry (`instruction`, `input`, `output`, `tags`, `source`, `license`)
- âœ… Excel-based bulk import and conversion to Hugging Face-compatible `.jsonl`
- âœ… Markdown generation from PDF books (using `pymupdf4llm`)
- âœ… Metadata extraction from PDF files including MD5 hash, authorship, license, page count, etc.
- âœ… JSONL output suitable for fine-tuning instruction-tuned LLMs (`<thinking>`)
- âœ… Multilingual dataset support with metadata traceability
- ğŸ› ï¸ CLI and Web UI for extending datasets interactively

---

## ğŸ“¦ Dataset Format

The dataset is stored in `.jsonl` format (one record per line), designed for instruction-tuned generative models like **Gemma 3B**, **Qwen**, or **Mistral**.

### Example Entry:

```json
{
  "instruction": "ErklÃ¤re die Unterschiede zwischen TCP und UDP.",
  "input": "",
  "output": "TCP ist verbindungsorientiert ...",
  "tags": ["Netzwerktechnik", "Informatik"],
  "source": "OpenBook: Netzwerke leicht erklÃ¤rt",
  "license": "CC-BY-SA 4.0",
  "created_at": "2025-06-03T22:26:29.342385+00:00"
}
````

---

## ğŸš€ Roadmap

1. ğŸ“ Manual and bulk dataset generation
2. ğŸª„ PDF conversion to Markdown
3. ğŸ“¤ Metadata export with hashing
4. ğŸ”„ Validation and deduplication tooling
5. ğŸ“¡ Hugging Face dataset upload
6. ğŸ§ª Evaluation pipeline for new examples
7. ğŸ”¬ Model fine-tuning (LoRA, QLoRA, DPO)
8. ğŸŒ Web frontend for dataset curation

---

## ğŸ“‚ Directory Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf/                # Source PDF books
â”‚   â”œâ”€â”€ markdown/           # Extracted Markdown files
â”‚   â”œâ”€â”€ dataset.jsonl       # Training data entries
â”‚   â””â”€â”€ pdf_metadata.jsonl  # Extracted PDF-metadata
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_dataset_entry.py
â”‚   â”œâ”€â”€ excel_to_dataset.py
â”‚   â”œâ”€â”€ pdf_to_markdown.py
â”‚   â””â”€â”€ generate_pdf_metadata.py
â”œâ”€â”€ README.md
â””â”€â”€ README-DE.md
```

---

## âœ… TODO

* [ ] Add Web UI for dataset entry and validation
* [ ] Add YAML â†’ JSONL converter for other model families
* [ ] Build HF-compatible `datasets` Python loader
* [ ] Add duplicate detector via MD5/file path
* [ ] Add license + source inference helper for bulk PDF imports
* [ ] Integration tests and schema validation
* [ ] Prepare test/train/validation split logic

